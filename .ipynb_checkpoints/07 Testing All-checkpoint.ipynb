{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "\n",
    "import re\n",
    "\n",
    "reload(sys)\n",
    "#sys.setdefaultencoding(\"ISO-8859-1\")\n",
    "sys.setdefaultencoding(\"UTF-8\")\n",
    "\n",
    "defaultFileNames = {'age': 'age-important-words-using-info-gain.txt',\n",
    "                    'gender': 'gender-important-words-using-info-gain.txt'\n",
    "                   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    review_text = BeautifulSoup(raw_text).get_text()\n",
    "    words = review_text.lower().split()\n",
    "    return(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_all_text(allText, numLines):\n",
    "    clean_train_data = []\n",
    "    for i in xrange(0, numLines):\n",
    "        clean_train_data.append(clean_text(allText[i]))\n",
    "    return clean_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featureSelection(train_x, task, train_y):\n",
    "    rows, cols = train_x.shape\n",
    "    top_info_words_numbers = [100, 200, 300, 500, 700, 1000, 2000, 5000, 7000, 8000, 9000, 10000, cols-1]\n",
    "    top_info_words_numbers =  sorted(top_info_words_numbers, reverse=True)\n",
    "\n",
    "    feature_selection_result = {}\n",
    "    \n",
    "    task_to_filenames = {'age': ['age-important-words-using-info-gain.txt', 'age-important-words-using-gain-ratio.txt'],\n",
    "                 'gender': ['gender-important-words-info-gain.txt', 'gender-important-words-gain-ratio.txt']\n",
    "                }\n",
    "    \n",
    "    filenames = task_to_filenames[task]\n",
    "    for filename in filenames:\n",
    "        with open(filename) as f:\n",
    "            alist = [line.rstrip() for line in f]\n",
    "        all_indices_ranked = alist[0].split(',')\n",
    "        all_indices_ranked = [int(x) for x in all_indices_ranked]\n",
    "        all_indices_ranked = [x-1 for x in all_indices_ranked]\n",
    "        \n",
    "        list_of_scores = []\n",
    "        for num_info_words in top_info_words_numbers:\n",
    "            clf = svm.SVC(kernel='linear', C=1)\n",
    "            scoring_function = 'accuracy'\n",
    "            \n",
    "            xx = [all_indices_ranked[x] for x in range(0, num_info_words)]\n",
    "            xx = tuple(xx)\n",
    "            smaller_train_x = train_x[:, xx]\n",
    "\n",
    "            scores = cross_validation.cross_val_score(clf, smaller_train_x, train_y, cv=10, scoring=scoring_function)\n",
    "            list_of_scores.append(scores)\n",
    "            \n",
    "            feature_selection_result[filename] = list_of_scores\n",
    "    return feature_selection_result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doSVMwithRBF(smaller_train_x, train_y, task):\n",
    "    params = {'age': {'gammas': [10**-4, 10**-3, 10**-1, 1, 10**1, 10**3, 10**4],\n",
    "                      'C': [10**-4, 10**-3, 10**-1, 1, 10**1, 10**3, 10**4]},\n",
    "              'gender': {'gammas': [10**-4, 10**-3, 10**-1, 1, 10**1, 10**3, 10**4],\n",
    "                      'C': [10**-4, 10**-1, 1, 10**1, 10**4]}\n",
    "             }\n",
    "    \n",
    "    gammas = params[task]['gammas']\n",
    "    C = params[task]['C']\n",
    "    \n",
    "    list_of_scores = []\n",
    "    results_with_params = {}\n",
    "    \n",
    "    for g in gammas:\n",
    "        for one_C in C:\n",
    "            clf = svm.SVC(kernel='rbf', gamma=g, C=one_C)\n",
    "            scoring_function = 'accuracy'\n",
    "            scores = cross_validation.cross_val_score(clf, smaller_train_x, train_y, cv=10, scoring=scoring_function)\n",
    "            list_of_scores.append(scores)\n",
    "            label = str(g)+','+str(one_C)\n",
    "            results_with_params[label] = scores\n",
    "    \n",
    "    svm_rbf_result_list_of_scores = list_of_scores\n",
    "    \n",
    "    return svm_rbf_result_list_of_scores, results_with_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculatePValue(input_dictionary):\n",
    "    p_values_dictionary = {}\n",
    "    for each_key in input_dictionary.keys():\n",
    "        list_of_scores = input_dictionary[each_key]\n",
    "        i = range(0,len(list_of_scores))\n",
    "        list_of_pvalues = []\n",
    "        for x, i  in zip(list_of_scores,i):\n",
    "            z_stat, p_val = stats.ranksums(list_of_scores[0], x)\n",
    "            list_of_pvalues.append( p_val)\n",
    "        p_values_dictionary[each_key] = list_of_pvalues\n",
    "    return p_values_dictionary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracies(feature_selection_result):\n",
    "    accuracies_dictionary = {}\n",
    "    for each_key in feature_selection_result.keys():\n",
    "        list_of_accuracies = feature_selection_result[each_key]\n",
    "        accuracies = [a.mean() for a in list_of_accuracies]\n",
    "        accuracies_dictionary[each_key] = accuracies\n",
    "    return accuracies_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getListOfRankedFeatures(train_x, num_features, task, fileNames=defaultFileNames):\n",
    "    fileName = fileNames[task]\n",
    "    \n",
    "    with open(fileName) as f:\n",
    "        alist = [line.rstrip() for line in f]\n",
    "    all_indices_ranked = alist[0].split(',')\n",
    "    all_indices_ranked = [int(x) for x in all_indices_ranked]\n",
    "    all_indices_ranked = [x-1 for x in all_indices_ranked]    \n",
    "    xx = [all_indices_ranked[x] for x in range(0, num_features)]\n",
    "    xx = tuple(xx)\n",
    "    return xx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSmallerTrainingSet(train_x, task, num_features, fileNames=defaultFileNames):\n",
    "    xx = getListOfRankedFeatures(train_x, num_features, task, fileNames)\n",
    "    smaller_train_x = train_x[:, xx]    \n",
    "    return smaller_train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doSVMwithPoly(train_x, train_y, task):\n",
    "    params = {'age': {'degrees': [1,2,3],\n",
    "                      'C': [10**-4, 10**-3, 10**-1, 1, 10**1, 10**3, 10**4]},\n",
    "              'gender': {'degrees': [1,2,3],\n",
    "                      'C': [10**-4, 10**-1, 1, 10**1, 10**4]}\n",
    "             }\n",
    "    \n",
    "    svm_poly_result = {}\n",
    "    \n",
    "    degrees = params[task]['degrees']\n",
    "    C = params[task]['C']\n",
    "    \n",
    "    list_of_scores = []\n",
    "    results_with_params = {}\n",
    "    for degree in degrees:\n",
    "        for one_C in C:\n",
    "            clf = svm.SVC(kernel='poly', degree=degree, coef0=one_C, gamma=1)\n",
    "            scoring_function = 'accuracy'\n",
    "            scores = cross_validation.cross_val_score(clf, smaller_train_x, train_y, cv=10, scoring=scoring_function)\n",
    "            list_of_scores.append(scores)\n",
    "            label = str(degree)+','+str(one_C)\n",
    "            results_with_params[label] = scores\n",
    "    \n",
    "    svm_poly_result_list_of_scores = list_of_scores\n",
    "    \n",
    "    return svm_poly_result_list_of_scores, results_with_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doFeatureWithResultsofOther(train_x, train_y_task, train_y_prior, task):\n",
    "    num_features_dict = {'age': 9000,\n",
    "                    'gender': 7000}\n",
    "    accuracies_dictionary = {}\n",
    "    \n",
    "    clf_age_classification = svm.SVC(kernel='poly', degree=3, coef0=10, gamma=1) \n",
    "    clf_gender_classification = svm.SVC(kernel='poly', degree=2, coef0=10000, gamma=1)\n",
    "\n",
    "    # age classification:\n",
    "    #    clf1 = svm.SVC(kernel='poly', degree=3, coef0=10, gamma=1)\n",
    "    \n",
    "    # gender classification:\n",
    "    #    clf2 = svm.SVC(kernel='poly', degree=2, coef0=10000, gamma=1)\n",
    "    \n",
    "    scoring_function = 'accuracy'\n",
    "    params = {'age': ['poly', 3, 10, 1],\n",
    "              'gender': ['poly', 2, 10000, 1]}\n",
    "    if task == \"age\":\n",
    "        prior = \"gender\"\n",
    "        clf_prior = clf_gender_classification\n",
    "        clf_task = clf_age_classification\n",
    "    else:\n",
    "        prior = \"age\"\n",
    "        clf_prior = clf_age_classification\n",
    "        clf_task = clf_gender_classification\n",
    "\n",
    "    num_features_task = num_features_dict[task]\n",
    "    num_features_prior = num_features_dict[prior]\n",
    "    \n",
    "    smaller_training_set_for_prior = getSmallerTrainingSet(train_x, task, num_features_prior) \n",
    "    smaller_training_set_for_task = getSmallerTrainingSet(train_x, task, num_features_task)\n",
    "    \n",
    "    clf_prior.fit(smaller_training_set_for_prior, train_y_prior)\n",
    "    results_prior = clf_prior.predict(smaller_training_set_for_prior)\n",
    "    \n",
    "    combined = np.column_stack((smaller_training_set_for_task, results_prior))\n",
    "    \n",
    "    scores = cross_validation.cross_val_score(clf_task, combined, train_y_task, cv=10, scoring=scoring_function)\n",
    "    accuracies_dictionary[task] = scores\n",
    "    \n",
    "    return accuracies_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doSVMwithPreprocessedText(train, task, num_features):\n",
    "    accuracy_dictionary = {}\n",
    "    \n",
    "    newFileNames = {'age': 'new-age-important-words-using-info-gain.txt',\n",
    "                 'gender': 'new-gender-important-words-using-info-gain.txt'\n",
    "                }\n",
    "    \n",
    "    train_y = train[task]\n",
    "    \n",
    "    clean_train_data = []\n",
    "    urls = []\n",
    "    hashtags = []\n",
    "    num_text = train[\"text\"].size\n",
    "    for i in xrange( 0, num_text):\n",
    "        one_clean_line = clean_text( train[\"text\"][i] )\n",
    "        new_clean_line = \"\"\n",
    "        #replacing links\n",
    "        url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', one_clean_line)\n",
    "        for one_url in url:\n",
    "            new_clean_line = one_clean_line.replace(one_url, \" LINK_HERE \")\n",
    "            one_clean_line = new_clean_line\n",
    "        urls.append(url)\n",
    "    \n",
    "        hashtag = re.findall('#(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', one_clean_line)\n",
    "    \n",
    "        for one_hashtag in hashtag:\n",
    "            new_clean_line = one_clean_line.replace(one_hashtag, \" HASHTAG_HERE \")\n",
    "            one_clean_line = new_clean_line\n",
    "        \n",
    "        hashtags.append(hashtag)\n",
    "        clean_train_data.append( one_clean_line )\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None)\n",
    "    train_x = vectorizer.fit_transform(clean_train_data)\n",
    "    train_x = train_x.toarray()\n",
    "    \n",
    "    smaller_train_x = getSmallerTrainingSet(train_x, task, num_features, newFileNames)\n",
    "    \n",
    "    if task == \"age\":\n",
    "        clf = svm.SVC(kernel='poly', degree=3, coef0=10, gamma=1)\n",
    "    else:\n",
    "        clf = svm.SVC(kernel='poly', degree=2, coef0=10000, gamma=1)\n",
    "    \n",
    "    scores = cross_validation.cross_val_score(clf, smaller_train_x, train_y, cv=10, scoring='accuracy')\n",
    "    \n",
    "    accuracy_dictionary[task] = scores\n",
    "    return accuracy_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tasks = [\"age\", \"gender\"]\n",
    "allResults = {}\n",
    "\n",
    "datafile = \"summary-english-truth.txt\"\n",
    "train = pd.read_csv(datafile, header=0, delimiter=\"\\t\", quoting=1)\n",
    "clean_train_data = clean_all_text(train[\"text\"], train[\"text\"].size)\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None)\n",
    "train_x = vectorizer.fit_transform(clean_train_data)\n",
    "train_x = train_x.toarray()\n",
    "\n",
    "\n",
    "for task in tasks:\n",
    "    train_y = train[task]\n",
    "\n",
    "    # experiment 1.1: Feature Selection\n",
    "    feature_selection_result = featureSelection(train_x, task, train_y)\n",
    "    accuracies_dictionary = getAccuracies(feature_selection_result)\n",
    "    p_values_dictionary = calculatePValue(feature_selection_result)\n",
    "\n",
    "    # experiment 1.2: SVM Poly\n",
    "    num_features_dictionary = {'age': 9000,\n",
    "                               'gender': 7000\n",
    "                              }\n",
    "\n",
    "    smaller_train_x = getSmallerTrainingSet(train_x, task, num_features)\n",
    "\n",
    "    svm_poly_result_list_of_scores, svm_poly_results_with_params = doSVMwithPoly(smaller_train_x, train_y, task)\n",
    "    svm_poly_accuracies_dictionary = getAccuracies(svm_poly_results_with_params)\n",
    "\n",
    "    #  experiment 1.3: SVM RBF\n",
    "    svm_rbf_result_list_of_scores, svm_rbf_results_with_params = doSVMwithRBF(smaller_train_x, train_y, task)\n",
    "    svm_rbf_accuracies_dictionary = getAccuracies(svm_rbf_results_with_params)\n",
    "\n",
    "    # experiment 2 7000 gender + age info, 9000 age + gender info\n",
    "    \n",
    "    if task == \"age\":\n",
    "        prior = \"gender\"\n",
    "    else:\n",
    "        prior = \"age\"\n",
    "    \n",
    "    train_y_task = train[task]\n",
    "    train_y_prior = train[prior]\n",
    "    res_for_other = doFeatureWithResultsofOther(train_x, train_y_task, train_y_prior, task)\n",
    "\n",
    "    # experiment 3: turning hashtags/hyperlinks to HASHTAG_HERE and LINK_HERE\n",
    "    \n",
    "    res_with_preprocessed_text = doSVMwithPreprocessedText(train, task, num_features)\n",
    "    \n",
    "    \n",
    "#    doRandomForest()\n",
    "#    doBoosting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': array([ 0.70588235,  0.76470588,  0.86666667,  0.8       ,  0.73333333,\n",
       "         0.8       ,  0.73333333,  0.93333333,  0.78571429,  0.92857143])}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80515406162464986"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1,0.0001': array([ 0.47058824,  0.52941176,  0.73333333,  0.6       ,  0.53333333,\n",
       "         0.73333333,  0.66666667,  0.66666667,  0.5       ,  0.71428571]),\n",
       " '1,0.001': array([ 0.47058824,  0.52941176,  0.73333333,  0.6       ,  0.53333333,\n",
       "         0.73333333,  0.66666667,  0.66666667,  0.5       ,  0.71428571]),\n",
       " '1,0.1': array([ 0.47058824,  0.52941176,  0.73333333,  0.6       ,  0.53333333,\n",
       "         0.73333333,  0.66666667,  0.66666667,  0.5       ,  0.71428571]),\n",
       " '1,1': array([ 0.47058824,  0.52941176,  0.73333333,  0.6       ,  0.53333333,\n",
       "         0.73333333,  0.66666667,  0.66666667,  0.5       ,  0.71428571]),\n",
       " '1,10': array([ 0.47058824,  0.52941176,  0.73333333,  0.6       ,  0.53333333,\n",
       "         0.73333333,  0.66666667,  0.66666667,  0.5       ,  0.71428571]),\n",
       " '1,1000': array([ 0.47058824,  0.52941176,  0.73333333,  0.6       ,  0.53333333,\n",
       "         0.73333333,  0.66666667,  0.66666667,  0.5       ,  0.71428571]),\n",
       " '1,10000': array([ 0.47058824,  0.52941176,  0.73333333,  0.6       ,  0.53333333,\n",
       "         0.73333333,  0.66666667,  0.66666667,  0.5       ,  0.71428571]),\n",
       " '2,0.0001': array([ 0.47058824,  0.58823529,  0.73333333,  0.6       ,  0.53333333,\n",
       "         0.73333333,  0.66666667,  0.66666667,  0.5       ,  0.71428571]),\n",
       " '2,0.001': array([ 0.47058824,  0.58823529,  0.73333333,  0.6       ,  0.53333333,\n",
       "         0.73333333,  0.66666667,  0.66666667,  0.5       ,  0.71428571]),\n",
       " '2,0.1': array([ 0.47058824,  0.58823529,  0.73333333,  0.6       ,  0.53333333,\n",
       "         0.73333333,  0.66666667,  0.66666667,  0.5       ,  0.71428571]),\n",
       " '2,1': array([ 0.52941176,  0.58823529,  0.86666667,  0.73333333,  0.66666667,\n",
       "         0.8       ,  0.73333333,  0.8       ,  0.57142857,  0.78571429]),\n",
       " '2,10': array([ 0.76470588,  0.76470588,  0.86666667,  0.86666667,  0.73333333,\n",
       "         0.8       ,  0.86666667,  0.93333333,  0.57142857,  0.92857143]),\n",
       " '2,1000': array([ 0.76470588,  0.76470588,  0.86666667,  0.86666667,  0.73333333,\n",
       "         0.8       ,  0.86666667,  0.93333333,  0.57142857,  0.92857143]),\n",
       " '2,10000': array([ 0.76470588,  0.76470588,  0.86666667,  0.86666667,  0.73333333,\n",
       "         0.8       ,  0.86666667,  0.93333333,  0.57142857,  0.92857143]),\n",
       " '3,0.0001': array([ 0.41176471,  0.52941176,  0.53333333,  0.46666667,  0.33333333,\n",
       "         0.53333333,  0.53333333,  0.66666667,  0.42857143,  0.71428571]),\n",
       " '3,0.001': array([ 0.41176471,  0.52941176,  0.53333333,  0.46666667,  0.33333333,\n",
       "         0.53333333,  0.6       ,  0.66666667,  0.42857143,  0.71428571]),\n",
       " '3,0.1': array([ 0.47058824,  0.58823529,  0.73333333,  0.53333333,  0.53333333,\n",
       "         0.8       ,  0.66666667,  0.6       ,  0.5       ,  0.64285714]),\n",
       " '3,1': array([ 0.64705882,  0.64705882,  0.86666667,  0.8       ,  0.73333333,\n",
       "         0.8       ,  0.8       ,  0.86666667,  0.57142857,  0.78571429]),\n",
       " '3,10': array([ 0.76470588,  0.76470588,  0.86666667,  0.86666667,  0.73333333,\n",
       "         0.8       ,  0.86666667,  0.93333333,  0.57142857,  0.92857143]),\n",
       " '3,1000': array([ 0.76470588,  0.76470588,  0.86666667,  0.86666667,  0.73333333,\n",
       "         0.8       ,  0.86666667,  0.93333333,  0.57142857,  0.92857143]),\n",
       " '3,10000': array([ 0.76470588,  0.76470588,  0.86666667,  0.86666667,  0.73333333,\n",
       "         0.8       ,  0.86666667,  0.93333333,  0.57142857,  0.92857143])}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_with_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-ebc3c5bc864b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmaller_training_set_for_task\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/numpy/lib/shape_base.pyc\u001b[0m in \u001b[0;36mcolumn_stack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "a = np.column_stack((smaller_training_set_for_task, prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
